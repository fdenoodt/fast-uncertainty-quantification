{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-12T12:13:21.854374Z",
     "start_time": "2024-10-12T12:13:21.838581Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import lightning as L\n",
    "import torch\n",
    "import torchvision.transforms.functional as TF\n",
    "from lightning import Trainer\n",
    "from torch import nn\n",
    "from torchvision.datasets import MNIST\n",
    "from torch import Tensor\n",
    "from typing import Tuple, Any\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from shared import overwrite_args_cli\n",
    "\n",
    "torch.set_float32_matmul_precision('medium')\n",
    "\n",
    "h = {  # hyperparameters\n",
    "    'dataset': 'MNIST',  #'TOY_REGRESSION', 'RADIO',\n",
    "    'dataset_path': '../data/',\n",
    "    \n",
    "    'num_views': 5,\n",
    "\n",
    "    'in_channels': 1,\n",
    "    'latent_dims': [32, 64],\n",
    "    'latent_strides': [2, 2],\n",
    "    'latent_kernel_sizes': [3, 3],\n",
    "    'num_classes': 10,\n",
    "\n",
    "    'epochs': 5,\n",
    "    'learning_rate': 1e-3,\n",
    "    'checkpoint_path': './saved_models',\n",
    "    'batch_size': 32,\n",
    "    'num_workers': 4,\n",
    "    \n",
    "    'limit_train_batches': 1.0,\n",
    "    'limit_val_batches': 1.0,\n",
    "    'limit_test_batches': 1.0,\n",
    "\n",
    "    'use_wandb': False,\n",
    "\n",
    "    'wandb_project': 'uncertainty',\n",
    "    'wandb_entity': 'oBoii',\n",
    "    'wandb_name': 'radio',\n",
    "\n",
    "    'train': True,  # Set this to false if you only want to evaluate the model\n",
    "    'fast_dev_run': False,\n",
    "    'overfit_batches': 0.0\n",
    "}\n",
    "\n",
    "h = overwrite_args_cli(h)"
   ],
   "id": "907486a9da6273f0",
   "outputs": [],
   "execution_count": 44
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-10-12T12:13:21.879069Z",
     "start_time": "2024-10-12T12:13:21.854374Z"
    }
   },
   "source": [
    "class Classifier(L.LightningModule):\n",
    "    def __init__(self, in_channels, latent_dims: list, latent_strides: list, latent_kernel_sizes: list,\n",
    "                 num_classes: int):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.latent_dims = latent_dims\n",
    "\n",
    "        # Encoder layers. Each layer will be a reparameterization layer\n",
    "        layers = []\n",
    "        for dim, stride, kernel_size in zip(latent_dims, latent_strides, latent_kernel_sizes):\n",
    "            layers.append(nn.Conv2d(in_channels, dim, kernel_size, stride))\n",
    "            in_channels = dim\n",
    "\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "        self.fc = nn.Linear(latent_dims[-1], num_classes)\n",
    "\n",
    "    def forward(self, x: Tensor):\n",
    "        # In: (batch, nb_views, C, H, W)\n",
    "        # Out: (batch, nb_views, num_classes)\n",
    "        assert x.dim() == 5, f\"Input shape is {x.shape}, expected 5 dimensions. (batch, nb_views, C, H, W)\"\n",
    "        return self.foward_multiple(x)\n",
    "\n",
    "    def forward_single(self, x):\n",
    "        # (batch, C, H, W)\n",
    "        assert x.dim() == 4, f\"Input shape is {x.shape}, expected 4 dimensions.\"\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "            x = nn.functional.relu(x)\n",
    "\n",
    "        # average pooling\n",
    "        x = nn.functional.avg_pool2d(x, x.size()[2:]).view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "    def foward_multiple(self, x):\n",
    "        \"\"\"Returns the predictions of all views of the input.\"\"\"\n",
    "        # in: (batch, nb_views, C, H, W)\n",
    "        # out: (batch, nb_views, num_classes)\n",
    "\n",
    "        # (batch, nb_views, C, H, W)\n",
    "        assert x.dim() == 5, f\"Input shape is {x.shape}, expected 5 dimensions.\"\n",
    "\n",
    "        batch, nb_views, c, h, w = x.shape\n",
    "        x = x.view(-1, c, h, w)  # (batch * nb_views, C, H, W)\n",
    "        x = self.forward_single(x)\n",
    "        x = x.view(batch, nb_views, -1)  # (batch, nb_views, num_classes)\n",
    "        return x\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch  # x: (batch, nb_views, C, H, W), y: (batch)\n",
    "        y = y.unsqueeze(1).expand(-1, x.size(1))  # (batch, nb_views)\n",
    "        y_hat = self(x) # (batch, nb_views, num_classes)\n",
    "        \n",
    "        # reshape to (batch * nb_views, num_classes)\n",
    "        y_hat = y_hat.view(-1, y_hat.size(2))\n",
    "        y = y.reshape(-1)\n",
    "        \n",
    "        loss = nn.functional.cross_entropy(y_hat, y)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y = y.unsqueeze(1).expand(-1, x.size(1))  # (batch, nb_views)\n",
    "        \n",
    "        y_hat = self(x)\n",
    "        \n",
    "        # reshape to (batch * nb_views, num_classes)\n",
    "        y_hat = y_hat.view(-1, y_hat.size(2))\n",
    "        y = y.reshape(-1)\n",
    "        \n",
    "        loss = nn.functional.cross_entropy(y_hat, y)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        y = y.unsqueeze(1).expand(-1, x.size(1))  # (batch, nb_views)\n",
    "        \n",
    "        # reshape to (batch * nb_views, num_classes)\n",
    "        y_hat = y_hat.view(-1, y_hat.size(2))\n",
    "        y = y.reshape(-1)\n",
    "        \n",
    "        loss = nn.functional.cross_entropy(y_hat, y)\n",
    "        acc = (y_hat.argmax(dim=1) == y).float().mean()\n",
    "        self.log('test_acc', acc)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=h['learning_rate'])\n",
    "\n",
    "    def save(self) -> None:\n",
    "        os.makedirs(h['checkpoint_path'], exist_ok=True)\n",
    "        torch.save(self.state_dict(), os.path.join(h['checkpoint_path'], 'model.pth'))\n",
    "\n",
    "    def load(self) -> 'SimpleNet':\n",
    "        self.load_state_dict(torch.load(os.path.join(h['checkpoint_path'], 'model.pth')))\n",
    "        self.eval()\n",
    "        return self\n",
    "\n",
    "\n",
    "def subsample(x: torch.Tensor, downscale_factor=2, noise_factor=0.05):\n",
    "    assert x.dim() == 3  # (C, H, W)\n",
    "\n",
    "    # Downsample using average pooling\n",
    "    x = F.avg_pool2d(x, kernel_size=downscale_factor)\n",
    "\n",
    "    # Introduce small random noise\n",
    "    noise = torch.randn_like(x) * noise_factor\n",
    "    x = x + noise\n",
    "\n",
    "    # Clip values to stay within the valid range (e.g., for MNIST this is [0, 1])\n",
    "    x = torch.clamp(x, 0.0, 1.0)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def subsample_with_random_stride(x: torch.Tensor, downscale_factor=2, max_offset=1):\n",
    "    assert x.dim() == 3  # (C, H, W)\n",
    "\n",
    "    # Randomly shift the starting point of the pooling window\n",
    "    offset_h = torch.randint(0, max_offset + 1, (1,)).item()\n",
    "    offset_w = torch.randint(0, max_offset + 1, (1,)).item()\n",
    "\n",
    "    # Apply padding to ensure pooling still works after shifting\n",
    "    x = F.pad(x, (offset_w, 0, offset_h, 0), mode='reflect')\n",
    "\n",
    "    # Downsample using average pooling\n",
    "    x = F.avg_pool2d(x, kernel_size=downscale_factor, stride=downscale_factor)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def subsample_with_patch_dropout(x: torch.Tensor, patch_size=2, dropout_prob=0.2):\n",
    "    assert x.dim() == 3  # (C, H, W)\n",
    "\n",
    "    # Divide the image into patches and randomly drop some\n",
    "    c, h, w = x.size()\n",
    "    patches_per_dim = h // patch_size\n",
    "    mask = torch.rand(patches_per_dim, patches_per_dim) > dropout_prob\n",
    "\n",
    "    # Reshape mask to match image resolution\n",
    "    mask = mask.repeat_interleave(patch_size, dim=0).repeat_interleave(patch_size, dim=1)\n",
    "    mask = mask.unsqueeze(0).expand(c, -1, -1)  # expand to channel dimension\n",
    "\n",
    "    # Apply the mask (dropout)\n",
    "    x = x * mask\n",
    "\n",
    "    # Downsample after patch dropout\n",
    "    x = F.avg_pool2d(x, kernel_size=patch_size)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def subsample_with_random_transform(x: torch.Tensor, downscale_factor=2):\n",
    "    assert x.dim() == 3  # (C, H, W)\n",
    "\n",
    "    # Random affine transformations (rotation, translation, scaling)\n",
    "    angle = torch.randn(1).item() * 45  # Random rotation (-10 to 10 degrees)\n",
    "    translate = [torch.randint(-2, 3, (1,)).item(), torch.randint(-2, 3, (1,)).item()]\n",
    "    scale = 1.0 + torch.randn(1).item() * 0.1  # Scale within [0.9, 1.1]\n",
    "\n",
    "    # Apply transformations (converted to PIL Image for easy manipulation)\n",
    "    x_pil = TF.to_pil_image(x.squeeze(0))  # Remove channel dim for MNIST (assumed grayscale)\n",
    "    x_pil = TF.affine(x_pil, angle=angle, translate=translate, scale=scale, shear=0)\n",
    "\n",
    "    # Convert back to tensor and add channel dim again\n",
    "    x = TF.to_tensor(x_pil).unsqueeze(0)\n",
    "\n",
    "    # Downsample using average pooling\n",
    "    x = F.avg_pool2d(x, kernel_size=downscale_factor)\n",
    "\n",
    "    # Remove channel dimension for imshow compatibility\n",
    "    x = x.squeeze(0)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def subsample_with_pixel_swapping(x: torch.Tensor, swap_prob=0.1, downscale_factor=2):\n",
    "    assert x.dim() == 3  # (C, H, W)\n",
    "\n",
    "    # Generate a mask for swapping pixels\n",
    "    c, h, w = x.size()\n",
    "    swap_mask = torch.rand(h, w) < swap_prob\n",
    "\n",
    "    # Add channel dimension to the mask\n",
    "    swap_mask = swap_mask.unsqueeze(0).expand(c, -1, -1)\n",
    "\n",
    "    # Shift the image by one pixel in a random direction and swap pixels according to the mask\n",
    "    shift_direction = torch.randint(0, 4, (1,)).item()  # Randomly pick direction: 0=up, 1=down, 2=left, 3=right\n",
    "    if shift_direction == 0:\n",
    "        x_shifted = F.pad(x[:, 1:, :], (0, 0, 0, 1), mode='reflect')\n",
    "    elif shift_direction == 1:\n",
    "        x_shifted = F.pad(x[:, :-1, :], (0, 0, 1, 0), mode='reflect')\n",
    "    elif shift_direction == 2:\n",
    "        x_shifted = F.pad(x[:, :, 1:], (0, 1, 0, 0), mode='reflect')\n",
    "    else:\n",
    "        x_shifted = F.pad(x[:, :, :-1], (1, 0, 0, 0), mode='reflect')\n",
    "\n",
    "    x[swap_mask] = x_shifted[swap_mask]\n",
    "\n",
    "    # Downsample after swapping\n",
    "    x = F.avg_pool2d(x, kernel_size=downscale_factor)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def rnd_subsample_method(x: torch.Tensor):\n",
    "    return subsample_with_random_transform(x)\n",
    "    # methods = [subsample,\n",
    "    #            subsample_with_random_stride,\n",
    "    #            subsample_with_patch_dropout,\n",
    "    #            subsample_with_random_transform,\n",
    "    #            subsample_with_pixel_swapping\n",
    "    #            ]\n",
    "    # return methods[torch.randint(0, len(methods), (1,)).item()](x)\n",
    "\n",
    "\n",
    "def create_multiple_views(x: torch.Tensor, num_views: int, transformation: callable):\n",
    "    return torch.stack([transformation(x) for _ in range(num_views)], dim=0)\n"
   ],
   "outputs": [],
   "execution_count": 45
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-12T12:13:21.983837Z",
     "start_time": "2024-10-12T12:13:21.880374Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def show_transformations():\n",
    "    # dispay the different subsampling methods, same image repeated 3 times\n",
    "    transform = transforms.Compose([transforms.ToTensor()])  #rnd_subsample_method])\n",
    "    mnist_train = MNIST(h['dataset_path'], train=True, download=True, transform=transform)\n",
    "\n",
    "    methods = [\n",
    "        # subsample,\n",
    "        #        subsample_with_random_stride,\n",
    "        #        subsample_with_patch_dropout,\n",
    "        subsample_with_random_transform,\n",
    "        # subsample_with_pixel_swapping\n",
    "    ]\n",
    "    x, y = mnist_train[0]\n",
    "    for method in methods:\n",
    "        # create 3 images and display them side by side\n",
    "        imgs = [method(x) for _ in range(3)]\n",
    "        imgs = torch.cat(imgs, dim=2)\n",
    "        plt.imshow(imgs.squeeze(0), cmap='gray')\n",
    "        plt.title(method.__name__)\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "show_transformations()"
   ],
   "id": "4d411989a300e9a4",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 46
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-10-12T12:13:21.985825Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def data_loaders(transformations: list):\n",
    "    transform = transforms.Compose([transforms.ToTensor()] + transformations)\n",
    "    mnist_train = MNIST(h['dataset_path'], train=True, download=True, transform=transform)\n",
    "    mnist_val = MNIST(h['dataset_path'], train=False, download=True, transform=transform)\n",
    "\n",
    "    train_loader = DataLoader(mnist_train, batch_size=h['batch_size'], shuffle=True)\n",
    "    val_loader = DataLoader(mnist_val, batch_size=h['batch_size'], shuffle=False)\n",
    "    return train_loader, val_loader\n",
    "\n",
    "\n",
    "train_loader, val_loader = data_loaders([lambda _: create_multiple_views(_, h['num_views'], subsample_with_random_transform)])\n",
    "# train_loader, val_loader = data_loaders([subsample_with_random_transform])\n",
    "\n",
    "c = Classifier(\n",
    "    in_channels=h['in_channels'],\n",
    "    latent_dims=h['latent_dims'],\n",
    "    latent_strides=h['latent_strides'],\n",
    "    latent_kernel_sizes=h['latent_kernel_sizes'],\n",
    "    num_classes=h['num_classes'],\n",
    ")\n",
    "\n",
    "trainer = Trainer(max_epochs=h['epochs'], fast_dev_run=h['fast_dev_run'], overfit_batches=h['overfit_batches'])\n",
    "\n",
    "if h['train']:\n",
    "    trainer.fit(c, train_loader, val_loader)\n",
    "\n",
    "# Evaluate the model\n",
    "results = trainer.test(c, val_loader)\n",
    "\n",
    "# Print accuracy\n",
    "print(f\"Test Accuracy: {results[0]['test_acc']:.4f}\")"
   ],
   "id": "38f85f6fd1a1f6af",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name   | Type       | Params\n",
      "--------------------------------------\n",
      "0 | layers | ModuleList | 18.8 K\n",
      "1 | fc     | Linear     | 650   \n",
      "--------------------------------------\n",
      "19.5 K    Trainable params\n",
      "0         Non-trainable params\n",
      "19.5 K    Total params\n",
      "0.078     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "429543ecc0c74af08efcc513d8bb1f68"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6814de8f57484d08ba64dca42336ed24"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fa2dabd16ca44b968c3c0bc282d4822e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "\n",
    "def _variances_vs_accuracy_per_input_img(classifier: Classifier, batch: Tuple[Tensor, Tensor]) -> Tensor:\n",
    "    \"\"\"Returns tensor of shape (batch, 2),\n",
    "    where the first column is the variance and the second column\n",
    "    is 1 or 0 if the prediction is correct or not.\"\"\"\n",
    "\n",
    "    x, y = batch\n",
    "    assert x.dim() == 5  # (batch, nb_views, C, H, W)\n",
    "\n",
    "    # 1) per view, compute the predictions\n",
    "    predictions = classifier.foward_multiple(x)  # (batch_size, num_views, num_classes)\n",
    "    assert len(predictions.shape) == 3, f\"Predictions shape is {predictions.shape}, expected 3 dimensions.\"\n",
    "\n",
    "    # 2) compute the variance of the predictions\n",
    "    variance = predictions.var(dim=1)  # (batch_size, num_classes)\n",
    "    average_variance = variance.mean(dim=1)  # (batch_size)\n",
    "\n",
    "    # 3) get mean of the predictions and take argmax\n",
    "    final_predictions = predictions.mean(dim=1).argmax(dim=1)  # (batch_size)\n",
    "\n",
    "    # 4) compute the accuracy of the mode with the labels\n",
    "    accuracy = (final_predictions == y)  # (batch_size)\n",
    "\n",
    "    # 5) stack the variance and accuracy\n",
    "    stack = torch.stack((average_variance, accuracy), dim=1)  # (batch_size, 2)\n",
    "    return stack\n",
    "\n",
    "\n",
    "def variances_vs_accuracy_per_input_img(classifier: Classifier, data_loader: DataLoader) -> Tensor:\n",
    "    \"\"\"Returns tensor of shape (batch, 2),\n",
    "    where the first column is the variance and the second column\n",
    "    is 1 or 0 if the prediction is correct or not.\"\"\"\n",
    "\n",
    "    classifier.eval()\n",
    "    var_vs_accuracy = None\n",
    "    for batch in data_loader:\n",
    "        batch_var_vs_accuracy = _variances_vs_accuracy_per_input_img(classifier, batch)\n",
    "        if var_vs_accuracy is None:\n",
    "            var_vs_accuracy = batch_var_vs_accuracy\n",
    "        else:\n",
    "            var_vs_accuracy = torch.cat((var_vs_accuracy, batch_var_vs_accuracy), dim=0)\n",
    "\n",
    "    return var_vs_accuracy\n",
    "\n",
    "\n",
    "c.eval()\n",
    "train, val = data_loaders([lambda _: create_multiple_views(_, h['num_views'], subsample_with_random_transform)])\n",
    "res = variances_vs_accuracy_per_input_img(c, val)\n",
    "print(res.shape)"
   ],
   "id": "2d9a2c7348a3bc1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "def distribution_variances_per_wrong_or_correct_prediction(var_vs_accuracy: Tensor):\n",
    "    variances = var_vs_accuracy[:, 0].numpy()  # scalar values\n",
    "    accuracies = var_vs_accuracy[:, 1].numpy()  # 0 or 1s\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    colors = ['blue', 'red']  # Colors for correct and wrong predictions\n",
    "    labels = ['Correct Predictions', 'Wrong Predictions']\n",
    "\n",
    "    for i, use_correct_predictions in enumerate([1., 0.]):\n",
    "        indices = np.where(accuracies == use_correct_predictions)[0]\n",
    "        variances_filtered = variances[indices]\n",
    "        ax.hist(variances_filtered, bins=25, color=colors[i], alpha=0.5, label=labels[i])\n",
    "\n",
    "    ax.set_xlabel(\"Variance\")\n",
    "    ax.set_ylabel(\"Count\")\n",
    "    ax.set_title(\"Variance Distribution for Correct vs Wrong Predictions\")\n",
    "    ax.legend()\n",
    "\n",
    "    # Log the matplotlib figure to wandb\n",
    "    # if options.use_wandb:\n",
    "    #     wandb_section = get_wandb_audio_classific_key(opt, classifier_config)\n",
    "    #     wandb.log({f\"{wandb_section}_softmax/variance_distribution_combined\": wandb.Image(fig)})\n",
    "\n",
    "    plt.show()\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "distribution_variances_per_wrong_or_correct_prediction(res.detach())"
   ],
   "id": "a7b9902756f8d206",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "def distribution_variances_per_wrong_or_correct_prediction_normalized(var_vs_accuracy: Tensor):\n",
    "    variances = var_vs_accuracy[:, 0].numpy()  # scalar values\n",
    "    accuracies = var_vs_accuracy[:, 1].numpy()  # 0 or 1s\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    colors = ['blue', 'red']  # Colors for correct and wrong predictions\n",
    "    labels = ['Correct Predictions', 'Wrong Predictions']\n",
    "\n",
    "    for i, use_correct_predictions in enumerate([1., 0.]):\n",
    "        indices = np.where(accuracies == use_correct_predictions)[0]\n",
    "        variances_filtered = variances[indices]\n",
    "        ax.hist(variances_filtered, bins=25, color=colors[i], alpha=0.5, label=labels[i], density=True)\n",
    "\n",
    "    ax.set_xlabel(\"Variance\")\n",
    "    ax.set_ylabel(\"Density\")\n",
    "    ax.set_title(\"Variance Distribution for Correct vs Wrong Predictions\")\n",
    "    ax.legend()\n",
    "\n",
    "    # Log the matplotlib figure to wandb\n",
    "    # if options.use_wandb:\n",
    "    #     wandb_section = get_wandb_audio_classific_key(opt, classifier_config)\n",
    "    #     wandb.log({f\"{wandb_section}_softmax/variance_distribution_combined\": wandb.Image(fig)})\n",
    "\n",
    "    plt.show()\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "distribution_variances_per_wrong_or_correct_prediction_normalized(res.detach())\n"
   ],
   "id": "82622fb7fb81a841",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "\n",
    "# Function to add Gaussian noise\n",
    "def add_gaussian_noise(images, noise_factor):\n",
    "    noisy_images = images + noise_factor * torch.randn(*images.shape)\n",
    "    noisy_images = torch.clip(noisy_images, 0., 1.)\n",
    "    return noisy_images\n",
    "\n",
    "\n",
    "# Function to compute model's uncertainty (e.g., softmax variance)\n",
    "def compute_uncertainty(outputs):  # outputs: (batch_size, num_classes)\n",
    "    # Softmax the output to get probabilities\n",
    "    probs = F.softmax(outputs, dim=1)\n",
    "\n",
    "    # Compute variance (uncertainty) across the class probabilities\n",
    "    uncertainty = probs.var(dim=1)\n",
    "    return uncertainty\n",
    "\n",
    "\n",
    "def plot_uncertainty_vs_noise_level(model, test_loader):\n",
    "    model.eval()\n",
    "\n",
    "    # List to store noise levels and corresponding uncertainty values\n",
    "    noise_levels = np.linspace(0, 1, 10)\n",
    "    mean_uncertainties = []\n",
    "\n",
    "    for noise_factor in noise_levels:\n",
    "        all_uncertainties = []\n",
    "\n",
    "        for images, _ in test_loader:\n",
    "            noisy_images = add_gaussian_noise(images, noise_factor)\n",
    "            with torch.no_grad():\n",
    "                outputs = model(noisy_images)\n",
    "                uncertainty = compute_uncertainty(outputs)\n",
    "                all_uncertainties.append(uncertainty.mean().item())\n",
    "\n",
    "        mean_uncertainties.append(np.mean(all_uncertainties))\n",
    "\n",
    "    # Plotting the results\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(noise_levels, mean_uncertainties, marker='o', linestyle='-')\n",
    "    plt.title('Model Uncertainty under Distributional Shift (Gaussian Noise)')\n",
    "    plt.xlabel('Noise Level (std dev of Gaussian noise)')\n",
    "    plt.ylabel('Mean Uncertainty (Softmax Variance)')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_uncertainty_vs_noise_level(c, val_loader)\n"
   ],
   "id": "5908aff8c71714ee",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
