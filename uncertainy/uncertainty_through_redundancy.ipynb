{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-12T17:04:51.329311Z",
     "start_time": "2024-10-12T17:04:51.312953Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import lightning as L\n",
    "import torch\n",
    "import torchvision.transforms.functional as TF\n",
    "from lightning import Trainer\n",
    "from torch import nn\n",
    "from torchvision.datasets import MNIST\n",
    "from torch import Tensor\n",
    "from typing import Tuple, Any\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from shared import overwrite_args_cli\n",
    "\n",
    "torch.set_float32_matmul_precision('medium')\n",
    "\n",
    "h = {  # hyperparameters\n",
    "    'dataset': 'MNIST',  #'TOY_REGRESSION', 'RADIO',\n",
    "    'dataset_path': '../data/',\n",
    "\n",
    "    'num_views': 5,\n",
    "    # 'loss': 'cross_entropy',  # 'cross_entropy', 'cross_entropy_with_distinct_path_regularization',\n",
    "    'alpha': 0.5, # 1 for cross_entropy, 0 for distinct_path_regularization\n",
    "\n",
    "    'in_channels': 1,\n",
    "    'latent_dims': [32, 64],\n",
    "    'latent_strides': [2, 2],\n",
    "    'latent_kernel_sizes': [3, 3],\n",
    "    'num_classes': 10,\n",
    "\n",
    "    'epochs': 2,\n",
    "    'learning_rate': 1e-3,\n",
    "    'checkpoint_path': './saved_models',\n",
    "    'batch_size': 32,\n",
    "    'num_workers': 4,\n",
    "\n",
    "    'limit_train_batches': 1.0,\n",
    "    'limit_val_batches': 1.0,\n",
    "    'limit_test_batches': 1.0,\n",
    "\n",
    "    'use_wandb': False,\n",
    "\n",
    "    'wandb_project': 'uncertainty',\n",
    "    'wandb_entity': 'oBoii',\n",
    "    'wandb_name': 'radio',\n",
    "\n",
    "    'train': True,  # Set this to false if you only want to evaluate the model\n",
    "    'fast_dev_run': False,\n",
    "    'overfit_batches': 0.0\n",
    "}\n",
    "\n",
    "h = overwrite_args_cli(h)\n",
    "# assert h['loss'] in ['cross_entropy', 'cross_entropy_with_distinct_path_regularization']"
   ],
   "id": "907486a9da6273f0",
   "outputs": [],
   "execution_count": 214
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-12T17:04:51.344962Z",
     "start_time": "2024-10-12T17:04:51.329311Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def subsample(x: torch.Tensor, downscale_factor=2, noise_factor=0.05):\n",
    "    assert x.dim() == 3  # (C, H, W)\n",
    "\n",
    "    # Downsample using average pooling\n",
    "    x = F.avg_pool2d(x, kernel_size=downscale_factor)\n",
    "\n",
    "    # Introduce small random noise\n",
    "    noise = torch.randn_like(x) * noise_factor\n",
    "    x = x + noise\n",
    "\n",
    "    # Clip values to stay within the valid range (e.g., for MNIST this is [0, 1])\n",
    "    x = torch.clamp(x, 0.0, 1.0)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def subsample_with_random_stride(x: torch.Tensor, downscale_factor=2, max_offset=1):\n",
    "    assert x.dim() == 3  # (C, H, W)\n",
    "\n",
    "    # Randomly shift the starting point of the pooling window\n",
    "    offset_h = torch.randint(0, max_offset + 1, (1,)).item()\n",
    "    offset_w = torch.randint(0, max_offset + 1, (1,)).item()\n",
    "\n",
    "    # Apply padding to ensure pooling still works after shifting\n",
    "    x = F.pad(x, (offset_w, 0, offset_h, 0), mode='reflect')\n",
    "\n",
    "    # Downsample using average pooling\n",
    "    x = F.avg_pool2d(x, kernel_size=downscale_factor, stride=downscale_factor)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def subsample_with_patch_dropout(x: torch.Tensor, patch_size=2, dropout_prob=0.2):\n",
    "    assert x.dim() == 3  # (C, H, W)\n",
    "\n",
    "    # Divide the image into patches and randomly drop some\n",
    "    c, h, w = x.size()\n",
    "    patches_per_dim = h // patch_size\n",
    "    mask = torch.rand(patches_per_dim, patches_per_dim) > dropout_prob\n",
    "\n",
    "    # Reshape mask to match image resolution\n",
    "    mask = mask.repeat_interleave(patch_size, dim=0).repeat_interleave(patch_size, dim=1)\n",
    "    mask = mask.unsqueeze(0).expand(c, -1, -1)  # expand to channel dimension\n",
    "\n",
    "    # Apply the mask (dropout)\n",
    "    x = x * mask\n",
    "\n",
    "    # Downsample after patch dropout\n",
    "    x = F.avg_pool2d(x, kernel_size=patch_size)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def subsample_with_random_transform(x: torch.Tensor, downscale_factor=2):\n",
    "    assert x.dim() == 3  # (C, H, W)\n",
    "\n",
    "    # Random affine transformations (rotation, translation, scaling)\n",
    "    angle = torch.randn(1).item() * 45  # Random rotation (-10 to 10 degrees)\n",
    "    translate = [torch.randint(-2, 3, (1,)).item(), torch.randint(-2, 3, (1,)).item()]\n",
    "    scale = 1.0 + torch.randn(1).item() * 0.1  # Scale within [0.9, 1.1]\n",
    "\n",
    "    # Apply transformations (converted to PIL Image for easy manipulation)\n",
    "    x_pil = TF.to_pil_image(x.squeeze(0))  # Remove channel dim for MNIST (assumed grayscale)\n",
    "    x_pil = TF.affine(x_pil, angle=angle, translate=translate, scale=scale, shear=0)\n",
    "\n",
    "    # Convert back to tensor and add channel dim again\n",
    "    x = TF.to_tensor(x_pil).unsqueeze(0)\n",
    "\n",
    "    # Downsample using average pooling\n",
    "    x = F.avg_pool2d(x, kernel_size=downscale_factor)\n",
    "\n",
    "    # Remove channel dimension for imshow compatibility\n",
    "    x = x.squeeze(0)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def subsample_with_pixel_swapping(x: torch.Tensor, swap_prob=0.1, downscale_factor=2):\n",
    "    assert x.dim() == 3  # (C, H, W)\n",
    "\n",
    "    # Generate a mask for swapping pixels\n",
    "    c, h, w = x.size()\n",
    "    swap_mask = torch.rand(h, w) < swap_prob\n",
    "\n",
    "    # Add channel dimension to the mask\n",
    "    swap_mask = swap_mask.unsqueeze(0).expand(c, -1, -1)\n",
    "\n",
    "    # Shift the image by one pixel in a random direction and swap pixels according to the mask\n",
    "    shift_direction = torch.randint(0, 4, (1,)).item()  # Randomly pick direction: 0=up, 1=down, 2=left, 3=right\n",
    "    if shift_direction == 0:\n",
    "        x_shifted = F.pad(x[:, 1:, :], (0, 0, 0, 1), mode='reflect')\n",
    "    elif shift_direction == 1:\n",
    "        x_shifted = F.pad(x[:, :-1, :], (0, 0, 1, 0), mode='reflect')\n",
    "    elif shift_direction == 2:\n",
    "        x_shifted = F.pad(x[:, :, 1:], (0, 1, 0, 0), mode='reflect')\n",
    "    else:\n",
    "        x_shifted = F.pad(x[:, :, :-1], (1, 0, 0, 0), mode='reflect')\n",
    "\n",
    "    x[swap_mask] = x_shifted[swap_mask]\n",
    "\n",
    "    # Downsample after swapping\n",
    "    x = F.avg_pool2d(x, kernel_size=downscale_factor)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def rnd_subsample_method(x: torch.Tensor):\n",
    "    return subsample_with_random_transform(x)\n",
    "    # methods = [subsample,\n",
    "    #            subsample_with_random_stride,\n",
    "    #            subsample_with_patch_dropout,\n",
    "    #            subsample_with_random_transform,\n",
    "    #            subsample_with_pixel_swapping\n",
    "    #            ]\n",
    "    # return methods[torch.randint(0, len(methods), (1,)).item()](x)\n",
    "\n",
    "\n",
    "def create_multiple_views(x: torch.Tensor, num_views: int, transformation: callable):\n",
    "    return torch.stack([transformation(x) for _ in range(num_views)], dim=0)"
   ],
   "id": "eb449dc621c36321",
   "outputs": [],
   "execution_count": 215
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-12T17:04:51.360674Z",
     "start_time": "2024-10-12T17:04:51.344962Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Loss(L.LightningModule):\n",
    "    def __init__(self, alpha: float):\n",
    "        super(Loss, self).__init__()\n",
    "        assert 0.0 <= alpha <= 1.0, f\"Alpha must be in [0, 1], got {alpha}.\"\n",
    "\n",
    "        self.cross_entropy = nn.CrossEntropyLoss()\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def forward(self, y_hat: Tensor, y: Tensor, latent_representations: list, nb_views: int):\n",
    "        y = y.unsqueeze(1).expand(-1, nb_views)  # (batch, nb_views)\n",
    "\n",
    "        # reshape to (batch * nb_views, num_classes)\n",
    "        y_hat = y_hat.view(-1, y_hat.size(2))\n",
    "        y = y.reshape(-1)\n",
    "\n",
    "        if self.alpha == 1.0:\n",
    "            return self.cross_entropy(y_hat, y)\n",
    "        else:\n",
    "            return (\n",
    "                    self.alpha * self.cross_entropy(y_hat, y) +\n",
    "                    (1 - self.alpha) * self.distinct_path_regularization(latent_representations))\n",
    "\n",
    "        # return (self.cross_entropy(y_hat, y) +\n",
    "        #         self.distinct_path_regularization(latent_representations))\n",
    "\n",
    "    # L2 distance between all pairs of views\n",
    "    # def distinct_path_regularization(self, latent_representations):\n",
    "    #     \"\"\"Compute the regularization term to penalize similar representations.\n",
    "    #     In: List of latent representations of shape (batch, nb_views, C, H, W)\n",
    "    #     Out: Scalar\n",
    "    #     \"\"\"\n",
    "    #     reg_loss = 0.0\n",
    "    #     for l in latent_representations:\n",
    "    #         assert l.dim() == 5, f\"Latent representation shape is {l.shape}, expected 5 dimensions.\"\n",
    "    #         # Compute pairwise distances between views\n",
    "    #         batch, nb_views, *dims = l.shape\n",
    "    #         l = l.view(batch, nb_views, -1)  # Flatten spatial dimensions\n",
    "    #         \n",
    "    #         distances = torch.cdist(l, l, p=2)  # Pairwise L2 distances\n",
    "    #         reg_loss += distances.mean()\n",
    "    #     return reg_loss\n",
    "\n",
    "    def distinct_path_regularization(self, latent_representations):\n",
    "        \"\"\"Compute the regularization term to penalize similar representations.\n",
    "        In: List of latent representations of shape (batch, nb_views, C, H, W)\n",
    "        Out: Scalar\n",
    "        \"\"\"\n",
    "        reg_loss = 0.0\n",
    "        for l in latent_representations:\n",
    "            assert l.dim() == 5, f\"Latent representation shape is {l.shape}, expected 5 dimensions.\"\n",
    "            # Compute pairwise dot products between views\n",
    "            batch, nb_views, *dims = l.shape\n",
    "            l = l.view(batch, nb_views, -1)  # Flatten spatial dimensions\n",
    "\n",
    "            # Normalize the vectors\n",
    "            l = F.normalize(l, p=2, dim=-1)\n",
    "\n",
    "            # Compute pairwise dot products\n",
    "            dot_products = torch.bmm(l, l.transpose(1, 2))  # (batch, nb_views, nb_views)\n",
    "\n",
    "            # We want to penalize high similarity, so we take the mean of the dot products\n",
    "            reg_loss += dot_products.mean()\n",
    "        return reg_loss\n",
    "\n",
    "# eg.: \n",
    "# torch.manual_seed(0)\n",
    "# loss = Loss(0)\n",
    "# b = 1\n",
    "# v = 5\n",
    "# y_hat = torch.randn(b, v, 10)  # (batch, nb_views, num_classes)\n",
    "# y = torch.randint(0, 10, (b, v))  # (batch, nb_views)\n",
    "# latent_representations = [torch.randn(b, v, 64, 7, 7), torch.randn(b, v, 128, 3, 3)]\n",
    "# loss(y_hat, y, latent_representations)\n"
   ],
   "id": "e19bbd674d607813",
   "outputs": [],
   "execution_count": 216
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-10-12T17:04:51.380107Z",
     "start_time": "2024-10-12T17:04:51.362673Z"
    }
   },
   "source": [
    "class Classifier(L.LightningModule):\n",
    "    def __init__(self, in_channels, latent_dims: list, latent_strides: list, latent_kernel_sizes: list,\n",
    "                 num_classes: int, loss: Loss):\n",
    "        super(Classifier, self).__init__()\n",
    "\n",
    "        assert callable(loss), f\"Loss must be a callable function, got {loss}.\"\n",
    "\n",
    "        self.latent_dims = latent_dims\n",
    "        self.loss: Loss = loss\n",
    "\n",
    "        # Encoder layers. Each layer will be a reparameterization layer\n",
    "        layers = []\n",
    "        for dim, stride, kernel_size in zip(latent_dims, latent_strides, latent_kernel_sizes):\n",
    "            layers.append(nn.Conv2d(in_channels, dim, kernel_size, stride))\n",
    "            layers.append(nn.BatchNorm2d(dim))\n",
    "            in_channels = dim\n",
    "\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "        self.fc = nn.Linear(latent_dims[-1], num_classes)\n",
    "\n",
    "    def forward(self, x: Tensor):\n",
    "        \"\"\"Out: (batch, nb_views, num_classes), latent_representations\n",
    "        In: (batch, nb_views, C, H, W) \"\"\"\n",
    "        assert x.dim() == 5, f\"Input shape is {x.shape}, expected 5 dimensions. (batch, nb_views, C, H, W)\"\n",
    "        return self.forward_multiple(x)\n",
    "\n",
    "    def forward_single(self, x) -> Tuple[Tensor, list]:\n",
    "        \"\"\"\n",
    "        In: (batch, C, H, W)\n",
    "        Out: (batch, num_classes), latent_representations\n",
    "        \"\"\"\n",
    "        # (batch, C, H, W)\n",
    "        assert x.dim() == 4, f\"Input shape is {x.shape}, expected 4 dimensions.\"\n",
    "        latent_representations = []\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "            if isinstance(layer, nn.BatchNorm2d):\n",
    "                x = nn.functional.relu(x)\n",
    "            latent_representations.append(x)\n",
    "\n",
    "        # average pooling\n",
    "        x = nn.functional.avg_pool2d(x, x.size()[2:]).view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x, latent_representations\n",
    "\n",
    "    def forward_multiple(self, x) -> Tuple[Tensor, list]:\n",
    "        \"\"\"Returns the predictions and latent representations of all views of the input.\n",
    "        In: (batch, nb_views, C, H, W)\n",
    "        Out: (batch, nb_views, num_classes), latent_representations\n",
    "        \"\"\"\n",
    "\n",
    "        # (batch, nb_views, C, H, W)\n",
    "        assert x.dim() == 5, f\"Input shape is {x.shape}, expected 5 dimensions.\"\n",
    "\n",
    "        batch, nb_views, c, h, w = x.shape\n",
    "        x = x.view(-1, c, h, w)  # (batch * nb_views, C, H, W)\n",
    "        x, latent_representations = self.forward_single(x)\n",
    "        x = x.view(batch, nb_views, -1)  # (batch, nb_views, num_classes)\n",
    "\n",
    "        # Reshape latent representations\n",
    "        latent_representations = [l.view(batch, nb_views, *l.shape[1:]) for l in latent_representations]\n",
    "        return x, latent_representations\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch  # x: (batch, nb_views, C, H, W), y: (batch)\n",
    "        y_hat, latent_representations = self(x)  # (batch, nb_views, num_classes)\n",
    "\n",
    "        loss = self.loss(y_hat, y, latent_representations, x.size(1))\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat, latent_representations = self(x)\n",
    "\n",
    "        loss = self.loss(y_hat, y, latent_representations, x.size(1))\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch # x: (batch, nb_views, C, H, W), y: (batch)\n",
    "        y_hat, latent_representations = self(x) # y_hat: (batch, nb_views, num_classes)\n",
    "\n",
    "        loss = self.loss(y_hat, y, latent_representations, x.size(1))\n",
    "\n",
    "        # torch.Size([32, 5, 10]) torch.Size([32, 5])\n",
    "        # print(y_hat.shape, y.shape)\n",
    "        # acc = (y_hat.argmax(dim=1) == y).float().mean()\n",
    "        y_hat_mean = y_hat.mean(dim=1)\n",
    "        acc = (y_hat_mean.argmax(dim=1) == y).float().mean()\n",
    "        # acc = (y_hat_mean.argmax(dim=1) == y[:, 0]).float().mean()\n",
    "\n",
    "        self.log('test_acc', acc)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=h['learning_rate'])\n",
    "\n",
    "    def save(self) -> None:\n",
    "        os.makedirs(h['checkpoint_path'], exist_ok=True)\n",
    "        torch.save(self.state_dict(), os.path.join(h['checkpoint_path'], 'model.pth'))\n",
    "\n",
    "    def load(self) -> 'SimpleNet':\n",
    "        self.load_state_dict(torch.load(os.path.join(h['checkpoint_path'], 'model.pth')))\n",
    "        self.eval()\n",
    "        return self"
   ],
   "outputs": [],
   "execution_count": 217
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-12T17:04:51.478597Z",
     "start_time": "2024-10-12T17:04:51.381406Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def show_transformations():\n",
    "    # dispay the different subsampling methods, same image repeated 3 times\n",
    "    transform = transforms.Compose([transforms.ToTensor()])  #rnd_subsample_method])\n",
    "    mnist_train = MNIST(h['dataset_path'], train=True, download=True, transform=transform)\n",
    "\n",
    "    methods = [\n",
    "        # subsample,\n",
    "        #        subsample_with_random_stride,\n",
    "        #        subsample_with_patch_dropout,\n",
    "        subsample_with_random_transform,\n",
    "        # subsample_with_pixel_swapping\n",
    "    ]\n",
    "    x, y = mnist_train[0]\n",
    "    for method in methods:\n",
    "        # create 3 images and display them side by side\n",
    "        imgs = [method(x) for _ in range(3)]\n",
    "        imgs = torch.cat(imgs, dim=2)\n",
    "        plt.imshow(imgs.squeeze(0), cmap='gray')\n",
    "        plt.title(method.__name__)\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "show_transformations()"
   ],
   "id": "4d411989a300e9a4",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 218
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-10-12T17:04:51.478597Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def data_loaders(transformations: list):\n",
    "    transform = transforms.Compose([transforms.ToTensor()] + transformations)\n",
    "    mnist_train = MNIST(h['dataset_path'], train=True, download=True, transform=transform)\n",
    "    mnist_val = MNIST(h['dataset_path'], train=False, download=True, transform=transform)\n",
    "\n",
    "    train_loader = DataLoader(mnist_train, batch_size=h['batch_size'], shuffle=True)\n",
    "    val_loader = DataLoader(mnist_val, batch_size=h['batch_size'], shuffle=False)\n",
    "    return train_loader, val_loader\n",
    "\n",
    "\n",
    "train_loader, val_loader = data_loaders(\n",
    "    [lambda _: create_multiple_views(_, h['num_views'], subsample_with_random_transform)])\n",
    "# train_loader, val_loader = data_loaders([subsample_with_random_transform])\n",
    "\n",
    "c = Classifier(\n",
    "    in_channels=h['in_channels'],\n",
    "    latent_dims=h['latent_dims'],\n",
    "    latent_strides=h['latent_strides'],\n",
    "    latent_kernel_sizes=h['latent_kernel_sizes'],\n",
    "    num_classes=h['num_classes'],\n",
    "    loss=Loss(h['alpha'])\n",
    ")\n",
    "\n",
    "trainer = Trainer(max_epochs=h['epochs'], fast_dev_run=h['fast_dev_run'], overfit_batches=h['overfit_batches'])\n",
    "\n",
    "if h['train']:\n",
    "    trainer.fit(c, train_loader, val_loader)\n",
    "\n",
    "# Evaluate the model\n",
    "results = trainer.test(c, val_loader)\n",
    "\n",
    "# Print accuracy\n",
    "print(f\"Test Accuracy: {results[0]['test_acc']:.4f}\")"
   ],
   "id": "38f85f6fd1a1f6af",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name   | Type       | Params\n",
      "--------------------------------------\n",
      "0 | loss   | Loss       | 0     \n",
      "1 | layers | ModuleList | 19.0 K\n",
      "2 | fc     | Linear     | 650   \n",
      "--------------------------------------\n",
      "19.7 K    Trainable params\n",
      "0         Non-trainable params\n",
      "19.7 K    Total params\n",
      "0.079     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a211da290ce84073b6d2a9d145eb7ef1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9ff917d6530a4da3bf9f26aa363040eb"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "34fa8c9b9bbe4d8581dce5db587ec21c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "def _variances_vs_accuracy_per_input_img(classifier: Classifier, batch: Tuple[Tensor, Tensor]) -> Tensor:\n",
    "    \"\"\"Returns tensor of shape (batch, 2),\n",
    "    where the first column is the variance and the second column\n",
    "    is 1 or 0 if the prediction is correct or not.\"\"\"\n",
    "\n",
    "    x, y = batch\n",
    "    assert x.dim() == 5  # (batch, nb_views, C, H, W)\n",
    "\n",
    "    # 1) per view, compute the predictions\n",
    "    predictions, _ = classifier.forward_multiple(x)  # (batch_size, num_views, num_classes)\n",
    "    assert len(predictions.shape) == 3, f\"Predictions shape is {predictions.shape}, expected 3 dimensions.\"\n",
    "\n",
    "    # 2) compute the variance of the predictions\n",
    "    variance = predictions.var(dim=1)  # (batch_size, num_classes)\n",
    "    average_variance = variance.mean(dim=1)  # (batch_size)\n",
    "\n",
    "    # 3) get mean of the predictions and take argmax\n",
    "    final_predictions = predictions.mean(dim=1).argmax(dim=1)  # (batch_size)\n",
    "\n",
    "    # 4) compute the accuracy of the mode with the labels\n",
    "    accuracy = (final_predictions == y)  # (batch_size)\n",
    "\n",
    "    # 5) stack the variance and accuracy\n",
    "    stack = torch.stack((average_variance, accuracy), dim=1)  # (batch_size, 2)\n",
    "    return stack\n",
    "\n",
    "\n",
    "def variances_vs_accuracy_per_input_img(classifier: Classifier, data_loader: DataLoader) -> Tensor:\n",
    "    \"\"\"Returns tensor of shape (batch, 2),\n",
    "    where the first column is the variance and the second column\n",
    "    is 1 or 0 if the prediction is correct or not.\"\"\"\n",
    "\n",
    "    classifier.eval()\n",
    "    var_vs_accuracy = None\n",
    "    for batch in data_loader:\n",
    "        batch_var_vs_accuracy = _variances_vs_accuracy_per_input_img(classifier, batch)\n",
    "        if var_vs_accuracy is None:\n",
    "            var_vs_accuracy = batch_var_vs_accuracy\n",
    "        else:\n",
    "            var_vs_accuracy = torch.cat((var_vs_accuracy, batch_var_vs_accuracy), dim=0)\n",
    "\n",
    "    return var_vs_accuracy\n",
    "\n",
    "\n",
    "c.eval()\n",
    "train, val = data_loaders([lambda _: create_multiple_views(_, h['num_views'], subsample_with_random_transform)])\n",
    "res = variances_vs_accuracy_per_input_img(c,\n",
    "                                          val)  # (batch_size, 2) where 1st column is variance and 2nd is 1 or 0 if the prediction is correct or not\n",
    "print(res.shape)"
   ],
   "id": "2d9a2c7348a3bc1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "def distribution_variances_per_wrong_or_correct_prediction(var_vs_accuracy: Tensor):\n",
    "    variances = var_vs_accuracy[:, 0].numpy()  # scalar values\n",
    "    accuracies = var_vs_accuracy[:, 1].numpy()  # 0 or 1s\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    colors = ['blue', 'red']  # Colors for correct and wrong predictions\n",
    "    labels = ['Correct Predictions', 'Wrong Predictions']\n",
    "\n",
    "    for i, use_correct_predictions in enumerate([1., 0.]):\n",
    "        indices = np.where(accuracies == use_correct_predictions)[0]\n",
    "        variances_filtered = variances[indices]\n",
    "        ax.hist(variances_filtered, bins=25, color=colors[i], alpha=0.5, label=labels[i])\n",
    "\n",
    "    ax.set_xlabel(\"Variance\")\n",
    "    ax.set_ylabel(\"Count\")\n",
    "    ax.set_title(\"Variance Distribution for Correct vs Wrong Predictions\")\n",
    "    ax.legend()\n",
    "\n",
    "    # Log the matplotlib figure to wandb\n",
    "    # if options.use_wandb:\n",
    "    #     wandb_section = get_wandb_audio_classific_key(opt, classifier_config)\n",
    "    #     wandb.log({f\"{wandb_section}_softmax/variance_distribution_combined\": wandb.Image(fig)})\n",
    "\n",
    "    plt.show()\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "distribution_variances_per_wrong_or_correct_prediction(res.detach())"
   ],
   "id": "a7b9902756f8d206",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "def distribution_variances_per_wrong_or_correct_prediction_normalized(var_vs_accuracy: Tensor):\n",
    "    variances = var_vs_accuracy[:, 0].numpy()  # scalar values\n",
    "    accuracies = var_vs_accuracy[:, 1].numpy()  # 0 or 1s\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    colors = ['blue', 'red']  # Colors for correct and wrong predictions\n",
    "    labels = ['Correct Predictions', 'Wrong Predictions']\n",
    "\n",
    "    for i, use_correct_predictions in enumerate([1., 0.]):\n",
    "        indices = np.where(accuracies == use_correct_predictions)[0]\n",
    "        variances_filtered = variances[indices]\n",
    "        ax.hist(variances_filtered, bins=25, color=colors[i], alpha=0.5, label=labels[i], density=True)\n",
    "\n",
    "    ax.set_xlabel(\"Variance\")\n",
    "    ax.set_ylabel(\"Density\")\n",
    "    ax.set_title(\"Variance Distribution for Correct vs Wrong Predictions\")\n",
    "    ax.legend()\n",
    "\n",
    "    # Log the matplotlib figure to wandb\n",
    "    # if options.use_wandb:\n",
    "    #     wandb_section = get_wandb_audio_classific_key(opt, classifier_config)\n",
    "    #     wandb.log({f\"{wandb_section}_softmax/variance_distribution_combined\": wandb.Image(fig)})\n",
    "\n",
    "    plt.show()\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "distribution_variances_per_wrong_or_correct_prediction_normalized(res.detach())\n"
   ],
   "id": "82622fb7fb81a841",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "# Function to add Gaussian noise\n",
    "def add_gaussian_noise(images, noise_factor):\n",
    "    noisy_images = images + noise_factor * torch.randn(*images.shape)\n",
    "    noisy_images = torch.clip(noisy_images, 0., 1.)\n",
    "    return noisy_images\n",
    "\n",
    "\n",
    "# Function to compute model's uncertainty (e.g., softmax variance)\n",
    "def compute_uncertainty(outputs):  # outputs: (batch_size, num_classes)\n",
    "    # Softmax the output to get probabilities\n",
    "    probs = F.softmax(outputs, dim=1)\n",
    "\n",
    "    # Compute variance (uncertainty) across the class probabilities\n",
    "    uncertainty = probs.var(dim=1)\n",
    "    return uncertainty\n",
    "\n",
    "\n",
    "def plot_uncertainty_vs_noise_level(model, test_loader):\n",
    "    model.eval()\n",
    "\n",
    "    # List to store noise levels and corresponding uncertainty values\n",
    "    noise_levels = np.linspace(0, 1, 10)\n",
    "    mean_uncertainties = []\n",
    "\n",
    "    for noise_factor in noise_levels:\n",
    "        all_uncertainties = []\n",
    "\n",
    "        for images, _ in test_loader:\n",
    "            noisy_images = add_gaussian_noise(images, noise_factor)\n",
    "            with torch.no_grad():\n",
    "                outputs, _ = model(noisy_images)\n",
    "                uncertainty = compute_uncertainty(outputs)\n",
    "                all_uncertainties.append(uncertainty.mean().item())\n",
    "\n",
    "        mean_uncertainties.append(np.mean(all_uncertainties))\n",
    "\n",
    "    # Plotting the results\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(noise_levels, mean_uncertainties, marker='o', linestyle='-')\n",
    "    plt.title('Model Uncertainty under Distributional Shift (Gaussian Noise)')\n",
    "    plt.xlabel('Noise Level (std dev of Gaussian noise)')\n",
    "    plt.ylabel('Mean Uncertainty (Softmax Variance)')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_uncertainty_vs_noise_level(c, val_loader)\n"
   ],
   "id": "5908aff8c71714ee",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "3914ada15b223428",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
